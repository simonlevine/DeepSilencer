{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Overall\n",
    "\n",
    "- Find ATAC-seq peaks that indicate chromatin accessibility (for episome similarity)\n",
    "- "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### gkSVM\n",
    "\n",
    "\n",
    "Enhanced Regulatory Sequence Prediction Using Gapped k-mer Features\n",
    "Mahmoud Ghandi*, Dongwon Lee*, Morteza Mohammad-Noori, Michael A. Beer\n",
    "\n",
    "Oligomers of length k, or k-mers, are convenient and widely used features for modeling the properties and functions of DNA and protein sequences. However, k-mers suffer from the inherent limitation that if the parameter k is increased to resolve longer features, the probability of observing any specific k-mer becomes very small, and k-mer counts approach a binary variable, with most k-mers absent and a few present once. Thus, any statistical learning approach using k-mers as features becomes susceptible to noisy training set k-mer frequencies once k becomes large. To address this problem, we introduce alternative feature sets using gapped k-mers, a new classifier, gkm-SVM, and a general method for robust estimation of k-mer frequencies. To make the method applicable to large-scale genome wide applications, we develop an efficient tree data structure for computing the kernel matrix. We show that compared to our original kmer-SVM and alternative approaches, our gkm-SVM predicts functional genomic regulatory elements and tissue specific enhancers with significantly improved accuracy, increasing the precision by up to a factor of two. We then show that gkm-SVM consistently outperforms kmer-SVM on human ENCODE ChIP-seq datasets, and further demonstrate the general utility of our method using a Naive-Bayes classifier. Although developed for regulatory sequence analysis, these methods can be applied to any sequence classification problem.\n",
    "\n",
    "- !(GkmExplain)[https://watermark.silverchair.com/btz322.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAArwwggK4BgkqhkiG9w0BBwagggKpMIICpQIBADCCAp4GCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMqLxQrjOLx-P7mGHnAgEQgIICbx9-MGXNlRedsyx7_ZL9GJKxe50xZkwXMReHYdSrhiDunJ8zo_fhsNZtuwIkz5vDK8JNTPJxTjOAFtgTDWJD_IymNT3aV00p3LBEKI1gC6qbPdGlBHgWnIXG1dMy1W3B7BqAO3KVZKwnKseVPR-6fk8JUYAAEkaZ9uKTFxYp8AnGMv9kLOUMZ1_F7CNXLW43xQt80TgazjjsZEag2svCIwIOK3-Hus8kyji2UlEcqTy-dsLnGsAsZLDyTLRbDNiaYv1QJXZP6awTZyK5jIIh4sPFzjajEwOKVs4qag03Peb6b45HEJqzymgOzk9qvZfK3GReDTNkiHgYMVlTiX7beS9TG98kbLjgTxBYYeKxBVGeYj3Hf6q0GBpRRpJnFsmVTCvKJ94rO-5MQq-99KquxkrlhLT1Y2JI-hsRNOG5ZMix1vcJWTe27XF7w521nvUBVTSkMm7Gn8rHJEIS3MA8izl9nI0yD7r2dHQmacAFlt-XWLfjNzMZKJU1Ydbwc5vm1Xetgin-Pyg0xw6oOdz3DcuQL07WiA3rzwce3JqQq48F2VWKuYtD8Msi2013TsNe_QJE1cf8AwCPV2ucIFyLuwXWGu7n88nlmrCftOW-CHz1pubQizgCKJ9qnMmDVZXHx_zTxC81k59xyZ8ldT15bKpZLRUkZw66oOo2whuIlvrTL1p9Qjk5joZoMijFbFf4R6wGH3w0QtMfTK-jCHg8Zn-oVSBnHRdJ_pVqi_LhI712QHlnmbf_ezuXVKF7TSDBatnDc2Fx1GYkB2mCovUKT5rOYFfR0XuvgA0b2TIKSIxXfNgIUxVSDi39Zg4qWW1n]:\n",
    "- A full k-mer refers to a letter subsequence of length k—for example,\n",
    "AAGT is a full 4-mer. By contrast, a gapped k-mer refers to a subsequence containing k letters and some number of gaps—for example,\n",
    "A*AG*T is a gapped 4-mer containing 2 gaps (* is used to denote a\n",
    "gap). In the gkm-SVM implementation, the parameter l denotes the\n",
    "full length of the subsequences considered (including gaps), while k"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Deep Silencer\n",
    "\n",
    "\n",
    "- \"most of the silencers were collected from both the gkmSVM-based model and our DeepSilencer model. However, there are some unique silencers from DeepSilencer model. For example, chr3: 48,997,302-48,997,398 is a silencer predicted by DeepSilencer model, but not a silencer predicted by the gkmSVM-based model. And chr3: 48,997,266-48,997,466 that is a silencer validated in the human K562 cell line covers this predicted silencer.\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_lightning.loggers import WandbLogger  # newline 1\n",
    "# from pytorch_lightning import Trainer\n",
    "\n",
    "# wandb_logger = WandbLogger()  # newline 2\n",
    "# trainer = Trainer(logger=wandb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNADataset(Dataset):\n",
    "    \"\"\"DNA + Pytorch Dataset\"\"\"\n",
    "    def __init__(self, X, y, encoding='one-hot'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (np.array): Path to the url .npz file with annotations.\n",
    "        \"\"\"\n",
    "        if encoding==None:\n",
    "            self.X = X\n",
    "        else:\n",
    "            self.X = torch.LongTensor(self.embedding_encoding(X))\n",
    "        self.y = torch.FloatTensor(y) #binary labels\n",
    "            #NOTE: we need this to be a float to accommodate nn.BCEWithLogitsLoss() later to match logit input.\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'X': self.X[idx], 'y': self.y[idx]}\n",
    "        return sample\n",
    "\n",
    "    def embedding_encoding(self, seq_array: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        :param seq_array: np array of DNA sequences\n",
    "        :return: np array of integer encodings of input DNA sequences\n",
    "        \"\"\"\n",
    "        nuc2id = {'A' : 0, 'C' : 1, 'T' : 2, 'G' : 3}\n",
    "        N = len(seq_array)\n",
    "        M = len(seq_array[0]) #since we know each sequence is the same length\n",
    "\n",
    "        int_array = np.full((N,M),0)\n",
    "        print('Nucleotides -> Integers')\n",
    "        for seq_num, seq in enumerate(seq_array):\n",
    "            for seq_idx, nucleotide in enumerate(seq):\n",
    "                nuc_idx = nuc2id[nucleotide]\n",
    "                int_array[seq_num, seq_idx] = nuc2id[nucleotide]\n",
    "        return int_array\n",
    "\n",
    "\n",
    "class DNADataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str, batch_size:int=64,encoding='one-hot'):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_fpath = data_dir\n",
    "        self.encoding=encoding\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        all_data = np.load(self.data_fpath)\n",
    "\n",
    "        self.train = DNADataset(all_data['train_seq'],all_data['train_y'],self.encoding)\n",
    "        self.val   = DNADataset(all_data['train_seq'],all_data['train_y'],self.encoding)\n",
    "        self.test  = DNADataset(all_data['test_seq'], all_data['test_y'], self.encoding)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Loading Keras Weights (from DeepSilencer repo) into pytorch:\n",
    "- caveats: keras stores as `double`, pytorch as `float`.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\n",
    "        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # --------------------------\n",
    "        # REPLACE WITH YOUR OWN\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        # --------------------------\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # --------------------------\n",
    "        # REPLACE WITH YOUR OWN\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log('val_loss', loss)\n",
    "        # --------------------------\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # --------------------------\n",
    "        # REPLACE WITH YOUR OWN\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log('test_loss', loss)\n",
    "        # --------------------------\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  }
 ]
}